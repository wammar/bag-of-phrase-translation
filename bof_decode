#!/usr/bin/env python
import argparse
import sys
import models
import heapq
import io
import os
import multiprocessing as mp
from collections import namedtuple, defaultdict
from hash_string import hash_string, hash_merge
from oracle_reordering import get_oracle_reordering, get_best_translation
from utils import dot_product
import dwl_query
from dwl_query import get_dwl_score
from datetime import datetime

global OOV_LOGPROB, weights, Hypothesis, Node, tm, lm, ref_sents, NUM_PROC

parser = argparse.ArgumentParser(description='Simple phrase based decoder.')
parser.add_argument('-i', '--input', dest='input', default='data/dev/newstest2011.fr.tok.10', help='File containing sentences to translate (default=data/input)')
parser.add_argument('-r', '--ref', dest='ref', default='data/dev/newstest2011.en.tok.10', help='File containing reference translations')
parser.add_argument('-o', '--output', dest='output', default='data/bof_kbest', help='(output) File containing kbest bag-of-phrase translations')
parser.add_argument('-oracle1', '--oracle1', dest='oracle1', default='data/oracle_1best', help='(output) File containing 1-best oracle-reordered translations')
parser.add_argument('-oracleK', '--oracleK', dest='oracleK', default='data/oracle_kbest', help='(output) File containing kbest oracle-reordered translations. this can be used as input to PRO because it reserves the feature values of the hypothesis')
parser.add_argument('-t', '--translation-model', dest='tm', default='data/tm', help='File containing translation model (default=data/tm)')
parser.add_argument('-per-sent-grammar_dir', '--translation-model', dest='per_sent_grammar_dir', default='', help='directory which contains per-sentence grammar files for the sentences being translated. Those files can be generated by running the command "python -m cdec.sa.extract -c extract.ini -g dev.grammars -j 2 < dev.lc-tok.es-en > dev.lc-tok.es-en.sgm". When this argument is specified, it overrides the translation model argument.')
parser.add_argument('-kbest', '--kbest', dest='kbest', default=10, type=int, help='size of the kbest list')
parser.add_argument('-tm_prune', '--tm_prune', dest='tm_prune', default=100, type=int, help='size of the kbest list')
parser.add_argument('-n', '--num_sentences', dest='num_sents', default=sys.maxint, type=int, help='Number of sentences to decode (default=no limit)')
parser.add_argument('-l', '--language-model', dest='lm', default='data/nc.1.lm', help='File containing ARPA-format language model (default=data/lm)')
parser.add_argument('-v', '--verbose', dest='verbose', action='store_true', default=False, help='Verbose mode (default=off)')
parser.add_argument('-ncpu', '--ncpu', dest='ncpu', type=int, default=1, help='How many processes am I allowed to use')
parser.add_argument('-syn_prune', '--syn_prune', dest='syn_prune', type=bool, default=0, help='Don\'t build all combinations of child hypotheses')
parser.add_argument('-syn_prune_threshold', '--syn_prune_threshold', dest='syn_prune_threshold', type=bool, default=0, help='if synthetic pruning is enabled, when to decide to skip the rest of hypotheses?')

parser.add_argument('-w_fwd', dest='w_fwd', type=float, default=1, help='weight of the fwd feature')
parser.add_argument('-w_bwd', dest='w_bwd', type=float, default=1, help='weight of the bwd feature')
parser.add_argument('-w_fwd_lex', dest='w_fwd_lex', type=float, default=1, help='weight of the fwd_lex feature')
parser.add_argument('-w_bwd_lex', dest='w_bwd_lex', type=float, default=1, help='weight of the bwd_lex feature')
parser.add_argument('-w_dwl', dest='w_dwl', type=float, default=1, help='weight of the dwl feature')
parser.add_argument('-w_dwl_oov', dest='w_dwl_oov', type=float, default=1, help='weight of the dwl_oov feature')
parser.add_argument('-w_p_count', dest='w_p_count', type=float, default=1, help='weight of the p_count feature')
parser.add_argument('-w_t_count', dest='w_t_count', type=float, default=1, help='weight of the t_count feature')
parser.add_argument('-w_1lm', dest='w_1lm', type=float, default=1, help='weight of the 1lm feature')
parser.add_argument('-w_rep', dest='w_rep', type=float, default=1, help='discourage repetition of words in the target sentence')

opts = parser.parse_args()

spurious_tokens = set([u'a', u'an', u'the', u'and', u'\'s', u'\'re', u'.', u';', u'-', u'!', u'?', u'$', u'"', u'\'', u'&', u'*', u')', u'('])

def count_repetition(set1, set2):
  count = 0
  for t1 in set1:
    for t2 in set2:
      if t1 == t2 and t1 not in spurious_tokens:
        count += 1
  return count

def merge_features(h1, h2):
  tbr = {'fwd': h1.features['fwd'] + h2.features['fwd'],
         'bwd': h1.features['bwd'] + h2.features['bwd'],
         'fwd_lex': h1.features['fwd_lex'] + h2.features['fwd_lex'],
         'bwd_lex': h1.features['bwd_lex'] + h2.features['bwd_lex'],
         'dwl': h1.features['dwl'] + h2.features['dwl'],
         'dwl_oov': h1.features['dwl_oov'] + h2.features['dwl_oov'],
         'p_count': h1.features['p_count'] + h2.features['p_count'], 
         't_count': h1.features['t_count'] + h2.features['t_count'], 
         '1lm': h1.features['1lm'] + h2.features['1lm'],
         'rep': h1.features['rep'] + h2.features['rep'] + count_repetition(h1.token_set, h2.token_set)}
  return tbr

def draw(f, tree):
  UNIT_WIDTH = 6
  for span_len in reversed(range(1, len(f)+1)):
    sys.stderr.write(u'len={0} '.format(span_len))
    for i in range(span_len*UNIT_WIDTH/2):
      sys.stderr.write(u' ')
    for from_src_pos in range(0, len(f)-span_len+1):
      sys.stderr.write(u' /{0}\ '.format(len(tree[(from_src_pos, from_src_pos + span_len)].hyps)))
    sys.stderr.write(u'\n\n\n')

def extract_phrase_bag(f, hyp, pieces):
  if hyp.tgt_phrase != None:
#    pieces.append(' '.join(f[hyp.from_src_pos:hyp.to_src_pos]))
    pieces.append(hyp.tgt_phrase)
    return
  extract_phrase_bag(f, hyp.left_child, pieces)
  extract_phrase_bag(f, hyp.right_child, pieces)
  
def initialise_global_vars():
    
    global OOV_LOGPROB, weights, Hypothesis, Node, tm, lm, ref_sents, feature_ids

    OOV_LOGPROB = -10.0
    weights = {'fwd':opts.w_fwd,
               'bwd':opts.w_bwd,
               'fwd_lex':opts.w_fwd_lex,
               'bwd_lex':opts.w_bwd_lex,
               'dwl':opts.w_dwl,
               'dwl_oov':opts.w_dwl_oov,
               'p_count':opts.w_p_count,
               't_count':opts.w_t_count,
               '1lm':opts.w_1lm,
               'rep':opts.w_rep}
    feature_ids = ['fwd',                    
                   'bwd',                    
                   'fwd_lex',                   
                   'bwd_lex',                    
                   'dwl',                   
                   'dwl_oov',                   
                   'p_count',                    
                   't_count',                    
                   '1lm',
                   'rep']
    Hypothesis = namedtuple('Hypothesis', 'logprob, features, from_src_pos, to_src_pos, tgt_phrase, left_child, right_child, hash, token_set')
    Node = namedtuple('Node', 'from_src_pos, to_src_pos, hyps')

    MAX_PHRASE_OPTIONS = opts.tm_prune
    
    tm = models.TM(opts.tm, MAX_PHRASE_OPTIONS, weights, opts.input)
    lm = models.LM(opts.lm)
    sys.stderr.write('Decoding %s...\n' % (opts.input,))
      
    ref_sents = [tuple(line.strip().split(' ||| ')) for line in io.open(opts.ref, encoding='utf8').readlines()[:opts.num_sents]] if opts.ref != "None" else None
      
def get_feature_list(feature_values_dict):
  assert(len(feature_values_dict) == len(feature_ids))
  feature_values_list = []
  for feature_id in feature_ids:
    feature_values_list.append(str(feature_values_dict[feature_id]))
  return feature_values_list

def translate(input_sent):
    
  global OOV_LOGPROB, weights, Hypothesis, Node, tm, lm, ref_sents
    
  f, sent_counter = input_sent
    
  sys.stderr.write(u'\n===========================================================================================\n')
  sys.stderr.write(u'now processing sent # {0}: {1}\n\n'.format(sent_counter, ' '.join(f)))
    
  # create a dictionary that maps (from_src_pos,to_src_pos) pairs to the corresponding Node
  tree = defaultdict(Node)
   
  # process lower cells first
  for span_len in range(1, len(f)+1):
    # process cells left to right
    #sys.stderr.write(u'processing span length {0}\n'.format(span_len))
    for from_src_pos in range(0, len(f)-span_len+1):
      # now, create the cell
      to_src_pos = from_src_pos+span_len
      #sys.stderr.write(u'processing cell tree[({0}, {1})]...'.format(from_src_pos, to_src_pos))
      tree[(from_src_pos, to_src_pos)] = Node(from_src_pos=from_src_pos, to_src_pos=to_src_pos, hyps=[])

      # src phrase in tm?
      if f[from_src_pos:to_src_pos] not in tm:
        # src phrase is oov?
        if span_len == 1:
          features = {'fwd':OOV_LOGPROB, 
                      'bwd':OOV_LOGPROB, 
                      'fwd_lex':OOV_LOGPROB, 
                      'bwd_lex':OOV_LOGPROB,
                      'dwl':dwl_query.OOV_LOG_PROB,
                      'dwl_oov':1.0,
                      'p_count':1.0,
                      't_count':1.0,
                      '1lm':OOV_LOGPROB,
                      'rep':0.0}
          tree[(from_src_pos, to_src_pos)].hyps.append( Hypothesis(logprob=dot_product(weights, features), 
                                                                   features=features,
                                                                   from_src_pos=from_src_pos,
                                                                   to_src_pos=to_src_pos,
                                                                   tgt_phrase=f[from_src_pos],
                                                                   left_child=None,
                                                                   right_child=None,
                                                                   hash=hash_string(f[from_src_pos]),
                                                                   token_set=set([ f[from_src_pos] ])))
          # end of oov handling 
        # not an oov, synthetic phrases will be added later
        else:
          pass
      # end of src phrase not in tm
      else:
        # add phrase pairs to the hypotheses
        for phrase in tm[f[from_src_pos:to_src_pos]]:
          _dwl = get_dwl_score(f, phrase.english) if weights['dwl'] != 0 else 0.0
          tgt_tokens = phrase.english.split()
          features = {'fwd':phrase.fwd, 
                      'bwd':phrase.bwd,
                      'fwd_lex':phrase.fwd_lex,
                      'bwd_lex':phrase.bwd_lex,
                      'dwl':_dwl,
                      'dwl_oov':1.0 if _dwl == dwl_query.OOV_LOG_PROB else 0.0,
                      'p_count':1.0,
                      't_count':len(tgt_tokens),
                      '1lm':lm.score_sequence(tgt_tokens),
                      'rep':0}
          tree[(from_src_pos, to_src_pos)].hyps.append( Hypothesis(logprob=dot_product(weights, features),
                                                                   features=features,
                                                                   from_src_pos=from_src_pos,
                                                                   to_src_pos=to_src_pos,
                                                                   tgt_phrase=phrase.english, 
                                                                   left_child=None, 
                                                                   right_child=None,
                                                                   hash=hash_string(phrase.english),
                                                                   token_set=set(tgt_tokens)))
      # end of src phrase in tm
      # end of tm lookup for src phrase

      # find the kbest synthetic hypotheses
      synthetic_hash_values=set()
      for mid_src_pos in range(from_src_pos + 1, to_src_pos):
        # for every way the current span can be split
        for left_hyp in tree[(from_src_pos, mid_src_pos)].hyps:
          for right_hyp in tree[(mid_src_pos, to_src_pos)].hyps:
            # consider all combinations 
            # TODO: you can stop after greedily combining kBEST hyps 
            features = merge_features(left_hyp, right_hyp)
            new_hyp = Hypothesis(logprob=dot_product(weights, features),
                                 features=features,
                                 from_src_pos=from_src_pos,
                                 to_src_pos=to_src_pos,
                                 tgt_phrase=None,
                                 left_child=left_hyp,
                                 right_child=right_hyp,
                                 hash=hash_merge(left_hyp.hash, right_hyp.hash),
                                 token_set=left_hyp.token_set|right_hyp.token_set)
            # TODO: instead of relying on the hash value only to determine duplicates, you can also rely on the logprob, after quantization
            # remove duplicate synthetic hypotheses
            if new_hyp.hash not in synthetic_hash_values:
              tree[(from_src_pos, to_src_pos)].hyps.append(new_hyp)
              synthetic_hash_values.add(new_hyp.hash)
              
      # now sort and prune
      tree[(from_src_pos, to_src_pos)].hyps.sort(reverse=True)
      #sys.stderr.write(u'created {0} hyps before pruning...'.format( len(tree[(from_src_pos, to_src_pos)].hyps) ))
      del tree[(from_src_pos, to_src_pos)].hyps[opts.kbest:]
      #sys.stderr.write(u'{0} hyps left after pruning.\n'.format( len(tree[(from_src_pos, to_src_pos)].hyps) ))
    # finished processing all cells with span_len
  # finished processing all spans
  
  # print tree
  #draw(f, tree)

  # now, spit out the kbest bag-of-phrase translations
  kbest_oracles = []
  sys.stderr.write(u'{0}-best translations:\n'.format(opts.kbest))
  bof_kbest_translations = [] # sentId ||| logprob ||| phrase1 phrase2 ...etc 
  oracle_kbest_translations = [] # sentId ||| tgtSentence ||| featureValue1 featureValue2 ...etc 
  oracle_1best_translation = None # sentence
  for hyp in tree[(0, len(f))].hyps:
    pieces = []
    extract_phrase_bag(f, hyp, pieces)
    bof_translation = u'{0} ||| {1} ||| {2}\n'.format(sent_counter, hyp.logprob, ' ||| '.join(pieces))
    bof_kbest_translations.append(bof_translation)
    sys.stderr.write(bof_translation)  
    if ref_sents:
      best_reordering = get_oracle_reordering(pieces, ref_sents[sent_counter])
      kbest_oracles.append(best_reordering)
      kbest_oracle_line = u'{0} ||| {1} ||| {2}\n'.format(sent_counter, best_reordering, ' '.join(get_feature_list(hyp.features)))
      oracle_kbest_translations.append(kbest_oracle_line)
      sys.stderr.write(best_reordering+u'\n')
  
  # now, out of the kbest-oracle-reorderings, select the one that would give the highest best bleu score
  if ref_sents:
    oracle_one_best = get_best_translation(kbest_oracles, ref_sents[sent_counter])
    oracle_1best_translation = u'{0}\n'.format(oracle_one_best)
    sys.stderr.write(u'the oracle_one_best translation is: {0}\n'.format(oracle_one_best))

  return (bof_kbest_translations, oracle_kbest_translations, oracle_1best_translation)
    
def bof_decode():
  NUM_PROC = opts.ncpu
  #input sentence now contains (input_sent, sent_num)
  input_sents = [(tuple(line.strip().split()), lineNum) for lineNum, line in enumerate(io.open(opts.input, encoding='utf8').readlines()[:opts.num_sents])]
    
  pool = mp.Pool(NUM_PROC, initialise_global_vars)
  with io.open(opts.output, encoding='utf8', mode='w') as bof_kbest_file, \
        io.open(opts.oracleK, encoding='utf8', mode='w') as oracle_kbest_file, \
        io.open(opts.oracle1, encoding='utf8', mode='w') as oracle_1best_file:
    for (bof_kbest, oracle_kbest, oracle_1best) in pool.imap(translate, input_sents):
      for line in bof_kbest:
        bof_kbest_file.write(line)
      for line in oracle_kbest:
        oracle_kbest_file.write(line)
      if oracle_1best:
        oracle_1best_file.write(oracle_1best)
      
if __name__=='__main__': 
  bof_decode()
