#!/usr/bin/env python
import argparse
import sys
import models
import heapq
import io
import os
import multiprocessing as mp
from collections import namedtuple, defaultdict
from hash_string import hash_string, hash_merge
from oracle_reordering import get_oracle_reordering, get_best_translation
from utils import dot_product
import dwl_query
from dwl_query import get_dwl_score
from datetime import datetime

global OOV_LOGPROB, weights, Hypothesis, Node, tm, lm, ref_sents, NUM_PROC

parser = argparse.ArgumentParser(description='Simple phrase based decoder.')
parser.add_argument('-i', '--input', dest='input', default='data/dev/newstest2011.fr.tok.10', help='File containing sentences to translate (default=data/input)')
parser.add_argument('-r', '--ref', dest='ref', default='data/dev/newstest2011.en.tok.10', help='File containing reference translations')
parser.add_argument('-o', '--output', dest='output', default='data/bof_kbest', help='(output) File containing kbest bag-of-phrase translations')
parser.add_argument('-oracle1', '--oracle1', dest='oracle1', default='data/oracle_1best', help='(output) File containing 1-best oracle-reordered translations')
parser.add_argument('-oracleK', '--oracleK', dest='oracleK', default='data/oracle_kbest', help='(output) File containing kbest oracle-reordered translations. this can be used as input to PRO because it reserves the feature values of the hypothesis')
parser.add_argument('-per-sent-grammar_dir', '--translation-model', dest='per_sent_grammar_dir', default='', help='directory which contains per-sentence grammar files for the sentences being translated. Those files can be generated by running the command "python -m cdec.sa.extract -c extract.ini -g dev.grammars -j 2 < dev.lc-tok.es-en > dev.lc-tok.es-en.sgm". When this argument is specified, it overrides the translation model argument.')
parser.add_argument('-kbest', '--kbest', dest='kbest', default=10, type=int, help='size of the kbest list')
parser.add_argument('-tm_prune', '--tm_prune', dest='tm_prune', default=100, type=int, help='size of the kbest list')
parser.add_argument('-n', '--num_sentences', dest='num_sents', default=sys.maxint, type=int, help='Number of sentences to decode (default=no limit)')
parser.add_argument('-l', '--language-model', dest='lm', default='data/nc.1.lm', help='File containing ARPA-format language model (default=data/lm)')
parser.add_argument('-v', '--verbose', dest='verbose', action='store_true', default=False, help='Verbose mode (default=off)')
parser.add_argument('-ncpu', '--ncpu', dest='ncpu', type=int, default=1, help='How many processes am I allowed to use')
parser.add_argument('-syn_prune', '--syn_prune', dest='syn_prune', type=bool, default=0, help='Don\'t build all combinations of child hypotheses')
parser.add_argument('-syn_prune_threshold', '--syn_prune_threshold', dest='syn_prune_threshold', type=bool, default=0, help='if synthetic pruning is enabled, when to decide to skip the rest of hypotheses?')

parser.add_argument('-w_EgivenFCoherent', dest='w_EgivenFCoherent', type=float, default=1, help='weight of the EgivenFCoherent feature')
parser.add_argument('-w_SampleCountF', dest='w_SampleCountF', type=float, default=1, help='weight of the SampleCountF feature')
parser.add_argument('-w_CountEF', dest='w_CountEF', type=float, default=1, help='weight of the CountEF feature')
parser.add_argument('-w_MaxLexFgivenE', dest='w_MaxLexFgivenE', type=float, default=1, help='weight of the MaxLexFgivenE feature')
parser.add_argument('-w_MaxLexEgivenF', dest='w_MaxLexEgivenF', type=float, default=1, help='weight of the MaxLexEgivenF feature')
parser.add_argument('-w_IsSingletonF', dest='w_IsSingletonF', type=float, default=1, help='weight of the IsSingletonF feature')
parser.add_argument('-w_IsSingletonFE', dest='w_IsSingletonFE', type=float, default=1, help='weight of the IsSingletonFE feature')
parser.add_argument('-w_dwl', dest='w_dwl', type=float, default=1, help='weight of the dwl feature')
parser.add_argument('-w_dwl_oov', dest='w_dwl_oov', type=float, default=1, help='weight of the dwl_oov feature')
parser.add_argument('-w_p_count', dest='w_p_count', type=float, default=1, help='weight of the p_count feature')
parser.add_argument('-w_t_count', dest='w_t_count', type=float, default=1, help='weight of the t_count feature')
parser.add_argument('-w_1lm', dest='w_1lm', type=float, default=1, help='weight of the 1lm feature')
parser.add_argument('-w_rep', dest='w_rep', type=float, default=1, help='discourage repetition of words in the target sentence')

opts = parser.parse_args()

spurious_tokens = set([u'a', u'an', u'the', u'and', u'\'s', u'\'re', u'.', u';', u'-', u'!', u'?', u'$', u'"', u'\'', u'&', u'*', u')', u'('])

def count_repetition(set1, set2):
  count = 0
  for t1 in set1:
    for t2 in set2:
      if t1 == t2 and t1 not in spurious_tokens:
        count += 1
  return count
                    
def merge_features(h1, h2):
  tbr = {}
  # all features can be aggregaged by addition
  for key in h1.features.keys():
    tbr[key] = h1.features[key] + h2.features[key]

  # repetition feature requires special handling
  tbr['rep'] += count_repetition(h1.token_set, h2.token_set)

  return tbr

def draw(f, tree):
  UNIT_WIDTH = 6
  for span_len in reversed(range(1, len(f)+1)):
    sys.stderr.write(u'len={0} '.format(span_len))
    for i in range(span_len*UNIT_WIDTH/2):
      sys.stderr.write(u' ')
    for from_src_pos in range(0, len(f)-span_len+1):
      sys.stderr.write(u' /{0}\ '.format(len(tree[(from_src_pos, from_src_pos + span_len)].hyps)))
    sys.stderr.write(u'\n\n\n')

def extract_phrase_bag(f, hyp, pieces):
  if hyp.tgt_phrase != None:
#    pieces.append(' '.join(f[hyp.from_src_pos:hyp.to_src_pos]))
    pieces.append(hyp.tgt_phrase)
    return
  extract_phrase_bag(f, hyp.left_child, pieces)
  extract_phrase_bag(f, hyp.right_child, pieces)
  
def initialise_global_vars():
    
    global OOV_LOGPROB, weights, Hypothesis, Node, tm, lm, ref_sents, feature_ids

    OOV_LOGPROB = -10.0
    weights = {'EgivenFCoherent':opts.w_EgivenFCoherent,
               'SampleCountF':opts.w_SampleCountF,
               'CountEF':opts.w_CountEF,
               'MaxLexFgivenE':opts.w_MaxLexFgivenE,
               'MaxLexEgivenF':opts.w_MaxLexEgivenF,
               'IsSingletonF':opts.w_IsSingletonF,
               'IsSingletonFE':opts.w_IsSingletonFE,
               'dwl':opts.w_dwl,
               'dwl_oov':opts.w_dwl_oov,
               'p_count':opts.w_p_count,
               't_count':opts.w_t_count,
               '1lm':opts.w_1lm,
               'rep':opts.w_rep}
    feature_ids = ['EgivenFCoherent',                    
                   'SampleCountF',                    
                   'CountEF',                   
                   'MaxLexFgivenE',
                   'MaxLexEgivenF',
                   'IsSingletonF',
                   'IsSingletonFE',
                   'dwl',                   
                   'dwl_oov',                   
                   'p_count',                    
                   't_count',                    
                   '1lm',
                   'rep']
    Hypothesis = namedtuple('Hypothesis', 'logprob, features, from_src_pos, to_src_pos, tgt_phrase, left_child, right_child, hash, token_set')
    Node = namedtuple('Node', 'from_src_pos, to_src_pos, hyps')

    MAX_PHRASE_OPTIONS = opts.tm_prune
    
    lm = models.LM(opts.lm)
    sys.stderr.write('Decoding %s...\n' % (opts.input,))
      
    ref_sents = [tuple(line.strip().split(' ||| ')) for line in io.open(opts.ref, encoding='utf8').readlines()[:opts.num_sents]] if opts.ref != "None" else None
      
def get_feature_list(feature_values_dict):
  assert(len(feature_values_dict) == len(feature_ids))
  feature_values_list = []
  for feature_id in feature_ids:
    feature_values_list.append(str(feature_values_dict[feature_id]))
  return feature_values_list

phrase = namedtuple("phrase", "english, EgivenFCoherent, SampleCountF, CountEF, MaxLexFgivenE, MaxLexEgivenF, IsSingletonF, IsSingletonFE")

def read_sent_grammar(sent_id):
  grammar_filename = '{}/grammar.{}'.format(opts.per_sent_grammar_dir, sent_id)
  grammar = {}
  
  for line in io.open(grammar_filename, encoding='utf8'):
    (x, f, e, features, alignment) = line.strip().split(" ||| ")
    f_tokens = f.strip().split()

    # if src or tgt contains a placeholder, skip this rule
    if '[X' in f or '[X' in e:
      continue

    #print f
    #print e

    # read the feature values
    features_dict = {}
    for feature in features.split():
      feature_name, feature_value = feature.split('=')
      feature_value = float(feature_value)
      features_dict[feature_name] = feature_value
      
    # phrase
    #assert 'EgivenFCoherent' in features_dict and 'SampleCountF' in features_dict and 'CountEF' in features_dict and 'MaxLexFgivenE' in features_dict and 'MaxLexEgivenF' in features_dict and 'IsSingletonF' in features_dict and 'IsSingletonFE' in features_dict
    p = phrase(e, 
               features_dict['EgivenFCoherent'], 
               features_dict['SampleCountF'], 
               features_dict['CountEF'], 
               features_dict['MaxLexFgivenE'], 
               features_dict['MaxLexEgivenF'], 
               features_dict['IsSingletonF'], 
               features_dict['IsSingletonFE'])
    
    f_tuple = tuple(f_tokens)
    if f_tuple in grammar:
      grammar[f_tuple].append(p)
    else:
      grammar[f_tuple] = [p]

  #sys.stderr.write( '{}\n'.format(grammar))
  return grammar

def translate(input_sent):
  
  print 'translate({}) starts'.format(input_sent[1])
  
  global OOV_LOGPROB, weights, Hypothesis, Node, lm, ref_sents
    
  f, sent_counter = input_sent
    
  # read the grammar for this sentence
  print 'before read_sent_grammar()'
  tm = read_sent_grammar(sent_counter)
  print 'after read_sent_grammar()'

  sys.stderr.write(u'\n===========================================================================================\n')
  sys.stderr.write(u'now processing sent # {0}: {1}\n\n'.format(sent_counter, ' '.join(f)))
    
  # create a dictionary that maps (from_src_pos,to_src_pos) pairs to the corresponding Node
  tree = defaultdict(Node)
   
  # process lower cells first
  for span_len in range(1, len(f)+1):
    # process cells left to right
    #sys.stderr.write(u'processing span length {0}\n'.format(span_len))
    for from_src_pos in range(0, len(f)-span_len+1):
      # now, create the cell
      to_src_pos = from_src_pos+span_len
      #sys.stderr.write(u'processing cell tree[({0}, {1})]...'.format(from_src_pos, to_src_pos))
      tree[(from_src_pos, to_src_pos)] = Node(from_src_pos=from_src_pos, to_src_pos=to_src_pos, hyps=[])

      # src phrase in tm?
      if f[from_src_pos:to_src_pos] not in tm:
        # src phrase is oov?
        if span_len == 1:
          features = {'EgivenFCoherent':0, 
                      'SampleCountF':0, 
                      'CountEF':0, 
                      'MaxLexFgivenE':0,
                      'MaxLexEgivenF':0,
                      'IsSingletonF':0,
                      'IsSingletonFE':0,
                      'dwl':0,
                      'dwl_oov':0,
                      'p_count':0,
                      't_count':0,
                      '1lm':0,
                      'rep':0}
          tree[(from_src_pos, to_src_pos)].hyps.append( Hypothesis(logprob=dot_product(weights, features), 
                                                                   features=features,
                                                                   from_src_pos=from_src_pos,
                                                                   to_src_pos=to_src_pos,
                                                                   tgt_phrase=f[from_src_pos],
                                                                   left_child=None,
                                                                   right_child=None,
                                                                   hash=hash_string(f[from_src_pos]),
                                                                   token_set=set([ f[from_src_pos] ])))
          # end of oov handling 
        # not an oov, synthetic phrases will be added later
        else:
          pass
      # end of src phrase not in tm
      else:
        # add phrase pairs to the hypotheses
        for phrase in tm[f[from_src_pos:to_src_pos]]:
          _dwl = 0
          #_dwl = get_dwl_score(f, phrase.english) if weights['dwl'] != 0 else 0.0
          tgt_tokens = phrase.english.split()
          features = {'EgivenFCoherent':phrase.EgivenFCoherent, 
                      'SampleCountF':phrase.SampleCountF, 
                      'CountEF':phrase.CountEF, 
                      'MaxLexFgivenE':phrase.MaxLexFgivenE,
                      'MaxLexEgivenF':phrase.MaxLexEgivenF,
                      'IsSingletonF':phrase.IsSingletonF,
                      'IsSingletonFE':phrase.IsSingletonFE,
                      'dwl':_dwl,
                      'dwl_oov':1.0 if _dwl == dwl_query.OOV_LOG_PROB else 0.0,
                      'p_count':1.0,
                      't_count':len(tgt_tokens),
                      '1lm':lm.score_sequence(tgt_tokens),
                      'rep':0}
          tree[(from_src_pos, to_src_pos)].hyps.append( Hypothesis(logprob=dot_product(weights, features),
                                                                   features=features,
                                                                   from_src_pos=from_src_pos,
                                                                   to_src_pos=to_src_pos,
                                                                   tgt_phrase=phrase.english, 
                                                                   left_child=None, 
                                                                   right_child=None,
                                                                   hash=hash_string(phrase.english),
                                                                   token_set=set(tgt_tokens)))
        # end of src phrase in tm
      # end of tm lookup for src phrase

      # find the kbest synthetic hypotheses
      synthetic_hash_values=set()
      for mid_src_pos in range(from_src_pos + 1, to_src_pos):
        # for every way the current span can be split
        for left_hyp in tree[(from_src_pos, mid_src_pos)].hyps:
          for right_hyp in tree[(mid_src_pos, to_src_pos)].hyps:
            # consider all combinations 
            # TODO: you can stop after greedily combining kBEST hyps 
            features = merge_features(left_hyp, right_hyp)
            new_hyp = Hypothesis(logprob=dot_product(weights, features),
                                 features=features,
                                 from_src_pos=from_src_pos,
                                 to_src_pos=to_src_pos,
                                 tgt_phrase=None,
                                 left_child=left_hyp,
                                 right_child=right_hyp,
                                 hash=hash_merge(left_hyp.hash, right_hyp.hash),
                                 token_set=left_hyp.token_set|right_hyp.token_set)
            # TODO: instead of relying on the hash value only to determine duplicates, you can also rely on the logprob, after quantization
            # remove duplicate synthetic hypotheses
            if new_hyp.hash not in synthetic_hash_values:
              tree[(from_src_pos, to_src_pos)].hyps.append(new_hyp)
              synthetic_hash_values.add(new_hyp.hash)
              
      # now sort and prune
      tree[(from_src_pos, to_src_pos)].hyps.sort(reverse=True)
      #sys.stderr.write(u'created {0} hyps before pruning...'.format( len(tree[(from_src_pos, to_src_pos)].hyps) ))
      del tree[(from_src_pos, to_src_pos)].hyps[opts.kbest:]
      #sys.stderr.write(u'{0} hyps left after pruning.\n'.format( len(tree[(from_src_pos, to_src_pos)].hyps) ))
    # finished processing all cells with span_len
  # finished processing all spans
  
  # print tree
  #draw(f, tree)

  # now, spit out the kbest bag-of-phrase translations
  kbest_oracles = []
  sys.stderr.write(u'{0}-best translations:\n'.format(opts.kbest))
  bof_kbest_translations = [] # sentId ||| logprob ||| phrase1 phrase2 ...etc 
  oracle_kbest_translations = [] # sentId ||| tgtSentence ||| featureValue1 featureValue2 ...etc 
  oracle_1best_translation = None # sentence
  for hyp in tree[(0, len(f))].hyps:
    pieces = []
    extract_phrase_bag(f, hyp, pieces)
    bof_translation = u'{0} ||| {1} ||| {2}\n'.format(sent_counter, hyp.logprob, ' ||| '.join(pieces))
    bof_kbest_translations.append(bof_translation)
    sys.stderr.write(bof_translation)  
    if ref_sents:
      best_reordering = get_oracle_reordering(pieces, ref_sents[sent_counter])
      kbest_oracles.append(best_reordering)
      kbest_oracle_line = u'{0} ||| {1} ||| {2}\n'.format(sent_counter, best_reordering, ' '.join(get_feature_list(hyp.features)))
      oracle_kbest_translations.append(kbest_oracle_line)
      sys.stderr.write(best_reordering+u'\n')
  
  # now, out of the kbest-oracle-reorderings, select the one that would give the highest best bleu score
  if ref_sents:
    oracle_one_best = get_best_translation(kbest_oracles, ref_sents[sent_counter])
    oracle_1best_translation = u'{0}\n'.format(oracle_one_best)
    sys.stderr.write(u'the oracle_one_best translation is: {0}\n'.format(oracle_one_best))

  return (bof_kbest_translations, oracle_kbest_translations, oracle_1best_translation)
    
def bof_decode():
  NUM_PROC = opts.ncpu
  #input sentence now contains (input_sent, sent_num)
  input_sents = [(tuple(line.strip().split()), lineNum) for lineNum, line in enumerate(io.open(opts.input, encoding='utf8').readlines()[:opts.num_sents])]
    
  pool = mp.Pool(NUM_PROC, initialise_global_vars)
  with io.open(opts.output, encoding='utf8', mode='w') as bof_kbest_file, \
        io.open(opts.oracleK, encoding='utf8', mode='w') as oracle_kbest_file, \
        io.open(opts.oracle1, encoding='utf8', mode='w') as oracle_1best_file:
    for X in pool.imap(translate, input_sents):
      (bof_kbest, oracle_kbest, oracle_1best) = X
      for line in bof_kbest:
        bof_kbest_file.write(line)
      for line in oracle_kbest:
        oracle_kbest_file.write(line)
      if oracle_1best:
        oracle_1best_file.write(oracle_1best)
      
if __name__=='__main__': 
  bof_decode()
